{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "**Clive Chan, 2017**\n",
    "\n",
    "It's a very simple algorithm, actually. You take a bunch of observed features individually and determine the probability of observing that feature given each of the possible predictions. You then multiply all of those together and see which prediction has the lowest probability. Bayes' theorem applied repeatedly, with an independence assumption.\n",
    "\n",
    "I'm going to be using a few \"prior\" classes, which basically streamingly update a model. Each will implement an `update` function, which takes a single observation and updates the internal model, and a `predict` function, which takes a potential observation and returns the probability that it will be observed, udner the current model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "class CounterModel:\n",
    "    def __init__(self):\n",
    "        self.totalCount = 0\n",
    "        self.counter = defaultdict(int)\n",
    "    def update(self, value):\n",
    "        self.counter[value] += 1\n",
    "        self.totalCount += 1\n",
    "    def predict(self, value):\n",
    "        return self.counter[value] / self.totalCount\n",
    "\n",
    "class NormalModel:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.sumSquaredDeviations = 0\n",
    "    def update(self, x):\n",
    "        self.n += 1\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta/self.n\n",
    "        self.sumSquaredDeviations += delta * (x - self.mean)\n",
    "    def predict(self, value):\n",
    "        # Directly using the PDF of normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements might include having `predict` return the log-likelihood rather than the raw likelihood, since that's somewhat more numerically stable for products of probabilities like this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
