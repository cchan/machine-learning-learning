{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "**Clive Chan, 2017**\n",
    "\n",
    "It's a very simple algorithm, actually. You take a bunch of observed features individually and determine the probability of observing that feature given each of the possible predictions. You then multiply all of those together and see which prediction has the lowest probability. Bayes' theorem applied repeatedly, with an independence assumption.\n",
    "\n",
    "I'm going to be using a few \"prior\" classes, which basically streamingly update a model. Each will implement an `update` function, which takes a single observation and updates the internal model, and a `predict` function, which takes a potential observation and returns the probability that it will be observed, udner the current model.\n",
    "\n",
    "## Models\n",
    "### Counter\n",
    "Just plain old counting and bucketing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from math import log\n",
    "class CounterModel:\n",
    "    def __init__(self):\n",
    "        self.totalCount = 0\n",
    "        self.counter = defaultdict(int)\n",
    "    def update(self, value):\n",
    "        self.counter[value] += 1\n",
    "        self.totalCount += 1\n",
    "    def predict(self, value):\n",
    "        return self.counter[value] / self.totalCount\n",
    "    def predict_log(self, value):\n",
    "        return log(self.counter[value]) - log(self.totalCount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal\n",
    "Assumes a normal prior.\n",
    "\n",
    "#### Proof of online variance algorithm [Welford's Algorithm] used below\n",
    "Following is based on [this](http://jonisalonen.com/2013/deriving-welfords-method-for-computing-variance/).\n",
    "\n",
    "We are calculating the sum of squared deviations (M2) in an online fashion, then dividing by the total count to get variance. Let's look at the difference between two consecutive M2s, which are indicated by $(N-1)s_N$ and $(N-2)s_{N-1}$ (since $s_N$ means sample standard deviation):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&(N-1)s_N - (N-2)s_{N-1} \\\\\n",
    "&= \\sum^{N}_{i=1} (x_i - \\bar{x}_N)^2 - \\sum^{N-1}_{i=1} (x_i - \\bar{x}_{N-1})^2 \\\\\n",
    "&= (x_N - \\bar{x}_N)^2 + \\sum^{N-1}_{i=1} (x_i - \\bar{x}_N)^2 - (x_i - \\bar{x}_{N-1})^2 \\\\\n",
    "&= (x_N - \\bar{x}_N)^2 + \\sum^{N-1}_{i=1} (x_i - \\bar{x}_N + x_i - \\bar{x}_{N-1})(x_i - \\bar{x}_N - x_i + \\bar{x}_{N-1}) \\\\\n",
    "&= (x_N - \\bar{x}_N)^2 + (- \\bar{x}_N + \\bar{x}_{N-1}) \\sum^{N-1}_{i=1} (2x_i - \\bar{x}_N - \\bar{x}_{N-1}) \\\\\n",
    "&= (x_N - \\bar{x}_N)^2 + (\\bar{x}_{N-1} - \\bar{x}_N) \\left( 2 \\sum^{N-1}_{i=1} x_i - (N-1)\\bar{x}_N - (N-1)\\bar{x}_{N-1} \\right) \\\\\n",
    "&= (x_N - \\bar{x}_N)^2 + (\\bar{x}_{N-1} - \\bar{x}_N) \\left( (N-1)\\bar{x}_{N-1} - N\\bar{x}_N + \\bar{x}_N \\right) \\\\\n",
    "&= (x_N - \\bar{x}_N)^2 + (\\bar{x}_{N-1} - \\bar{x}_N) \\left( \\bar{x}_N - x_N \\right) \\\\\n",
    "&= (x_N - \\bar{x}_N)((x_N - \\bar{x}_N) - (\\bar{x}_{N-1} - \\bar{x}_N)) \\\\\n",
    "&= (x_N - \\bar{x}_N)(x_N - \\bar{x}_{N-1}) \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This makes a single pass very easy! See line 10 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import exp, pi, sqrt\n",
    "\n",
    "class NormalModel:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.mean = 0\n",
    "        self.sumSquaredDeviations = 0\n",
    "    def update(self, x):\n",
    "        self.n += 1\n",
    "        delta = x - self.mean\n",
    "        self.mean += delta/self.n\n",
    "        self.sumSquaredDeviations += delta * (x - self.mean)\n",
    "    def predict(self, value):\n",
    "        variance = self.sumSquaredDeviations / (self.n - 1)\n",
    "        exponent = - (value - self.mean)**2 / (2 * variance)\n",
    "        return 1/sqrt(2 * pi * variance) * exp(exponent)\n",
    "    def predict_log(self, value):\n",
    "        variance = self.sumSquaredDeviations / (self.n - 1)\n",
    "        exponent = - (value - self.mean)**2 / (2 * variance)\n",
    "        return - 0.5 * log(2 * pi * variance) + exponent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, time for Naive Bayes. I'll follow the same kind of `update`, `predict`, `predict_log` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from functools import reduce\n",
    "import operator\n",
    "\n",
    "class NaiveBayes:\n",
    "    def __init__(self, models):\n",
    "        self.models = [defaultdict(lambda: model()) for model in models]\n",
    "        self.outputSet = set()\n",
    "    def update(self, inputs, output):\n",
    "        self.outputSet.add(output)\n",
    "        for x, model in zip(inputs, self.models):\n",
    "            model[output].update(x)\n",
    "    def predict(self, inputs):\n",
    "        # Modify this to actually return the probability?\n",
    "        return max(((reduce(lambda a, b: a * b,\n",
    "                   (model[output].predict(x) for x, model in zip(inputs, self.models))\n",
    "                  ), output) for output in self.outputSet), key=operator.itemgetter(0))[1]\n",
    "    def predict_log(self, inputs):\n",
    "        return max(( (sum(model[output].predict_log(x) for x, model in zip(inputs, self.models)), output)\n",
    "                    for output in self.outputSet), key=operator.itemgetter(0))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model on some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes([NormalModel, CounterModel])\n",
    "train_data = [\n",
    "    ([1,\"a\"], \"one\"),\n",
    "    ([6,\"b\"], \"two\"),\n",
    "    ([2,\"a\"], \"one\"),\n",
    "    ([5,\"b\"], \"two\"),\n",
    "    ([3,\"a\"], \"one\"),\n",
    "    ([4,\"b\"], \"two\"),\n",
    "]\n",
    "for datum in train_data:\n",
    "    nb.update(datum[0], datum[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n",
      "one\n"
     ]
    }
   ],
   "source": [
    "test_inputs = [\n",
    "    [-100, \"a\"],\n",
    "    [-3, \"a\"],\n",
    "    [-2, \"a\"],\n",
    "    [-1, \"a\"],\n",
    "    [0, \"a\"],\n",
    "    [1, \"a\"],\n",
    "    [2, \"a\"],\n",
    "    [3, \"a\"],\n",
    "    [4, \"a\"],\n",
    "    [5, \"a\"],\n",
    "    [6, \"a\"],\n",
    "    [+100, \"a\"],\n",
    "]\n",
    "for input in test_inputs:\n",
    "    print(nb.predict(input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results with ones and twos are confusing, I think I'm doing something wrong. Possibly:\n",
    "- Numerical instability with the far ends of the normal distribution?\n",
    "    - The problem is that using predict_log on counter doesn't make any sense. In fact, using the counter at all makes no sense - it will return a zero probability for anything it hasn't seen yet. Is there anything I can do about this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... in progress ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvements might include using `predict_log` (returning the log-likelihood rather than the raw likelihood), since that's somewhat more numerically stable for products of probabilities like this. But I haven't yet figured out a nice way of solving the $\\ln 0$ problem, since that introduces immense numerical instability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
